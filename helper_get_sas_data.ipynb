{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "087c7fb5-17dc-4248-8503-bf5bc500cd9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyreadstat\n",
    "\n",
    "# Specify the path to your SAS7BDAT file\n",
    "file_path = \"shared_data_read_only/Data/DHS/GH_2022_DHS/GHKR8BSD/GHKR8BFL.SAS7BDAT\"\n",
    "\n",
    "# Read the SAS7BDAT file into a DataFrame\n",
    "df, meta = pyreadstat.read_sas7bdat(file_path)\n",
    "\n",
    "header_map = {\"V013\": \"Age in 5-year groups \", \"V024\": \"Region\",\"V130\": \"Religion\", \"V131\":\"Ethnicity\", \"V133\":\"Education in single years\",\n",
    "             \"V136\":\"Number of household members\", \"V149\": \"Educational attainment\", \"V157\":\"Frequency of reading newspaper or magazine\",\"V158\":\"Frequency of listening to radio\",\n",
    "             \"V159\":\"Frequency of watching television\", \"V155\":\"Literacy\", \"V156\":\"Ever participated in a literacy program\", \"V171A\":\"Use of internet\", \"V171B\":\"Frequency of using internet last month\",\n",
    "             \"V169A\":\"Owns a mobile telephone\", \"V191A\":\"Wealth index factor score for urban/rural\", \"V190A\":\"Wealth index for urban/rural\", \"V191\":\"Wealth index factor score combined (5 decimals)\"}\n",
    "headers = ['V013', 'V024', 'V130', 'V131', 'V133', 'V136', 'V149', 'V157', 'V158', 'V159', 'V155', 'V156', 'V171A', 'V171B', 'V169A', 'V191A', 'V190A', 'V191']\n",
    "\n",
    "df_renamed = df[headers].rename(columns=header_map)\n",
    "df_renamed.to_csv(\"sas_2022_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5715124d-163d-4279-9108-1f9428409a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            CASEID  MIDX V000  V001  V002  V003  V004      V005  V006    V007  \\\n",
      "0          1  9  1   1.0  GH5   1.0   9.0   1.0   1.0  652967.0  10.0  2008.0   \n",
      "1          1 78  2   1.0  GH5   1.0  78.0   2.0   1.0  652967.0  10.0  2008.0   \n",
      "2          1 78  2   2.0  GH5   1.0  78.0   2.0   1.0  652967.0  10.0  2008.0   \n",
      "3          2 10  2   1.0  GH5   2.0  10.0   2.0   2.0  911245.0  11.0  2008.0   \n",
      "4          2 92  1   1.0  GH5   2.0  92.0   1.0   2.0  911245.0  11.0  2008.0   \n",
      "\n",
      "   ...  S1015M  S1016  S1017  S1018  S1019  S1020  S1021  S1022  S1119A  \\\n",
      "0  ...     1.0    0.0    3.0    5.0    7.0    2.0    5.0    3.0     NaN   \n",
      "1  ...     1.0    0.0    2.0    2.0    2.0    2.0    3.0    2.0     NaN   \n",
      "2  ...     1.0    0.0    2.0    2.0    2.0    2.0    3.0    2.0     NaN   \n",
      "3  ...     1.0    0.0    2.0    3.0    7.0    1.0    1.0    3.0     NaN   \n",
      "4  ...     NaN    0.0    3.0    3.0    4.0    2.0    0.0    0.0     NaN   \n",
      "\n",
      "   S1132A  \n",
      "0     0.0  \n",
      "1     NaN  \n",
      "2     NaN  \n",
      "3     8.0  \n",
      "4     NaN  \n",
      "\n",
      "[5 rows x 1009 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m remove_headers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV171A\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV171B\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV169A\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV191A\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV190A\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# headers not present in 2014 data\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m header \u001b[38;5;129;01min\u001b[39;00m remove_headers:    \n\u001b[0;32m---> 24\u001b[0m     \u001b[43mheaders\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m df_rename \u001b[38;5;241m=\u001b[39m df_2008[headers]\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39mheader_map)\n\u001b[1;32m     26\u001b[0m df_renamed\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msas_2008_df.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyreadstat\n",
    "\n",
    "# Specify the path to your SAS file\n",
    "file_path_2014 = \"shared_data_read_only/Data/DHS/GH_2014_DHS/GHKR72SD/GHKR72FL.SAS7BDAT\"\n",
    "\n",
    "def parse_map_file(file_path):\n",
    "    with open(file_path, 'r', encoding='windows-1252') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    data_items = []\n",
    "    for line in lines:\n",
    "        print(line)\n",
    "    \n",
    "    return data_items\n",
    "\n",
    "# parsed_data = parse_map_file(file_path)\n",
    "\n",
    "df_2014, meta = pyreadstat.read_sas7bdat(file_path_2014)\n",
    "\n",
    "print(df_2014.head())\n",
    "remove_headers = ['V171A', 'V171B', 'V169A', 'V191A', 'V190A'] # headers not present in 2014 data\n",
    "for header in remove_headers:\n",
    "    headers.remove(header)\n",
    "df_rename = df_2014[headers].rename(columns=header_map)\n",
    "df_renamed.to_csv(\"sas_2014_df.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f536756e-9d1d-4371-a744-c76952284d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            CASEID  MIDX V000  V001  V002  V003  V004      V005  V006    V007  \\\n",
      "0          1  9  1   1.0  GH5   1.0   9.0   1.0   1.0  652967.0  10.0  2008.0   \n",
      "1          1 78  2   1.0  GH5   1.0  78.0   2.0   1.0  652967.0  10.0  2008.0   \n",
      "2          1 78  2   2.0  GH5   1.0  78.0   2.0   1.0  652967.0  10.0  2008.0   \n",
      "3          2 10  2   1.0  GH5   2.0  10.0   2.0   2.0  911245.0  11.0  2008.0   \n",
      "4          2 92  1   1.0  GH5   2.0  92.0   1.0   2.0  911245.0  11.0  2008.0   \n",
      "\n",
      "   ...  S1015M  S1016  S1017  S1018  S1019  S1020  S1021  S1022  S1119A  \\\n",
      "0  ...     1.0    0.0    3.0    5.0    7.0    2.0    5.0    3.0     NaN   \n",
      "1  ...     1.0    0.0    2.0    2.0    2.0    2.0    3.0    2.0     NaN   \n",
      "2  ...     1.0    0.0    2.0    2.0    2.0    2.0    3.0    2.0     NaN   \n",
      "3  ...     1.0    0.0    2.0    3.0    7.0    1.0    1.0    3.0     NaN   \n",
      "4  ...     NaN    0.0    3.0    3.0    4.0    2.0    0.0    0.0     NaN   \n",
      "\n",
      "   S1132A  \n",
      "0     0.0  \n",
      "1     NaN  \n",
      "2     NaN  \n",
      "3     8.0  \n",
      "4     NaN  \n",
      "\n",
      "[5 rows x 1009 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyreadstat\n",
    "\n",
    "# Specify the path to your SAS file\n",
    "file_path_2008 = \"shared_data_read_only/Data/DHS/GH_2008_DHS/GHKR5ASD/GHKR5AFL.SAS7BDAT\"\n",
    "\n",
    "def parse_map_file(file_path):\n",
    "    with open(file_path, 'r', encoding='windows-1252') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    data_items = []\n",
    "    for line in lines:\n",
    "        print(line)\n",
    "    \n",
    "    return data_items\n",
    "\n",
    "# parsed_data = parse_map_file(file_path)\n",
    "df_2008, meta = pyreadstat.read_sas7bdat(file_path_2008)\n",
    "\n",
    "print(df_2008.head())\n",
    "remove_headers = ['V171A', 'V171B', 'V169A', 'V191A', 'V190A'] # headers not present in 2014 data\n",
    "for header in remove_headers:\n",
    "    if header in headers:\n",
    "        headers.remove(header)\n",
    "df_rename = df_2008[headers].rename(columns=header_map)\n",
    "df_renamed.to_csv(\"sas_2008_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2266e2-d09b-46e0-92f2-b0d8ead723ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
